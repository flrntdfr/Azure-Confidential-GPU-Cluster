{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d75d0ce",
   "metadata": {},
   "source": [
    "<!-- du4://thèse/cai/results.ipynb?d=20251024?loc=ttum?hPa=1020 -->\n",
    "\n",
    "# Confidential Artificial Intelligence: What's the Catch?\n",
    "### _Performance and costs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from functools import cached_property\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\", context=\"paper\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEE_Mode(Enum):\n",
    "    TEE_ON = \"tee_on\"\n",
    "    TEE_OFF = \"tee_off\"\n",
    "\n",
    "class Sorting(Enum):\n",
    "    LEXICOGRAPHGIC = \"lexicographic\"\n",
    "    NATURAL = \"narutal\"\n",
    "    MODEL_SIZE = \"model_size\"\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    path: Path\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.path.stem\n",
    "\n",
    "    @cached_property\n",
    "    def conditions(self) -> List[\"Condition\"]:\n",
    "        return [\n",
    "            Condition(\n",
    "                q, \n",
    "                q.parent.name, \n",
    "                TEE_Mode.TEE_ON if q.name == TEE_Mode.TEE_ON.value else TEE_Mode.TEE_OFF\n",
    "            )\n",
    "            for q in self.path.glob(\"*/*\")\n",
    "            if q.is_dir() and q.name in {TEE_Mode.TEE_ON.value, TEE_Mode.TEE_OFF.value}\n",
    "        ]\n",
    "    \n",
    "    def list_condition_names(self, sorting: Sorting):\n",
    "        match sorting:\n",
    "            case Sorting.MODEL_SIZE:\n",
    "                all_conditions_sorted = sorted(self.conditions, key=lambda c: int(c.model_size))\n",
    "                return list(dict.fromkeys(c.name for c in all_conditions_sorted))\n",
    "            case Sorting.NATURAL:\n",
    "                condition_names = list(dict.fromkeys(c.name for c in self.conditions))\n",
    "                return sorted(condition_names, key=self._natural_sort_key)\n",
    "            case _:\n",
    "                return list(dict.fromkeys(c.name for c in self.conditions))\n",
    "\n",
    "    def get_conditions(self, tee_mode: TEE_Mode):\n",
    "        return [c for c in self.conditions if c.tee_mode == tee_mode]\n",
    "\n",
    "    def get_condition(self, name: str, tee_mode: TEE_Mode):\n",
    "        return next(\n",
    "            filter(lambda c: c.name == name and c.tee_mode == tee_mode, self.conditions),\n",
    "            None,\n",
    "        )\n",
    "    \n",
    "    def get_all_runs(self):\n",
    "        return [r for c in self.conditions for r in c.runs]\n",
    "    \n",
    "    def get_runs(self, tee_mode: TEE_Mode):\n",
    "        return [r for c in self.conditions for r in c.runs if c.tee_mode == tee_mode]\n",
    "  \n",
    "    def _natural_sort_key(self, text: str):\n",
    "        return [int(c) if c.isdigit() else c.lower() for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "    def __str__(self):\n",
    "        name = self.name\n",
    "        nb_conditions = len(self.conditions)\n",
    "        nb_total_runs = len(self.get_all_runs())\n",
    "        return f\"Experiment: {name}, Conditions: {nb_conditions} ({nb_total_runs} total measurements)\"\n",
    "\n",
    "@dataclass\n",
    "class Condition:\n",
    "    path: Path\n",
    "    name: str\n",
    "    tee_mode: TEE_Mode\n",
    "\n",
    "    @property\n",
    "    def model_name(self) :\n",
    "        return self.path.parent.name.split(\"_\")\n",
    "\n",
    "    @property\n",
    "    def model_size(self) -> str:\n",
    "        return re.search(r\"(\\d+)[bB]\", \"_\".join(self.model_name)).group(1)\n",
    "\n",
    "    @cached_property\n",
    "    def runs(self) -> List[\"Run\"]:\n",
    "        run_paths = list(self.path.glob(\"*repetition_*\"))\n",
    "        json_files = sorted([r for r in run_paths if r.suffix == \".json\"])\n",
    "        csv_files = sorted([r for r in run_paths if r.suffix == \".csv\"])\n",
    "\n",
    "        assert len(list(run_paths)) > 0, \"Empty results\"\n",
    "        assert len(json_files) == len(csv_files), f\"Mismatch: {len(json_files)} .json vs. {len(csv_files)} .csv: {run_paths}\"\n",
    "\n",
    "        return [\n",
    "            Run(\n",
    "                idx, json_file, self.path / f\"{json_file.stem}_power_metrics.csv\"\n",
    "            )\n",
    "            for idx, json_file in enumerate(json_files)\n",
    "        ]\n",
    "\n",
    "    def get_all_runs(self) -> List[\"Run\"]:\n",
    "        return self.runs\n",
    "\n",
    "    def get_run(self, index: int) -> \"Run\":\n",
    "        return self.runs[index]\n",
    "\n",
    "    def get_median_throughput_with_std(self) -> Tuple[float, float]:\n",
    "        output_throughputs = [\n",
    "            rep.get_vllm_key(\"output_throughput\") for rep in self.runs\n",
    "        ]\n",
    "        return np.median(output_throughputs), np.std(output_throughputs)\n",
    "    \n",
    "    def get_median_ttft_with_std_and_p95(self):# -> Tuple[float, float, float]:\n",
    "        latencies = [\n",
    "            rep.get_vllm_key(\"ttfts\") for rep in self.runs \n",
    "        ]\n",
    "        #return np.median(latencies), np.std(ltencies), np.percentile(latencies, 95) # TODO: compare to VLLM output\n",
    "        return latencies\n",
    "\n",
    "    def get_median_itl_with_std_and_p95(self) -> Tuple[float, float, float]:\n",
    "        latencies = [\n",
    "            rep.get_vllm_key(\"itls\") for rep in self.runs\n",
    "        ]\n",
    "        return np.median(latencies), np.std(latencies), np.percentile(latencies, 95) # TODO: compare to VLLM output\n",
    "\n",
    "@dataclass\n",
    "class Run:\n",
    "    index: int\n",
    "    path_vllm_json: Path\n",
    "    path_power_csv: Path\n",
    "\n",
    "    @cached_property\n",
    "    def vllm_metrics(self) -> dict:\n",
    "        return json.loads(self.path_vllm_json.read_text())\n",
    "\n",
    "    @cached_property\n",
    "    def gpu_metrics(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.path_power_csv)\n",
    "\n",
    "    # Input\n",
    "        \n",
    "    @property\n",
    "    def model_id(self) -> str:\n",
    "        return self.vllm_metrics[\"model_id\"]\n",
    "\n",
    "    @property\n",
    "    def num_prompts(self) -> int:\n",
    "        return self.vllm_metrics[\"num_prompts\"]\n",
    "\n",
    "    @property\n",
    "    def input_len(self) -> list[int]:\n",
    "        return self.vllm_metrics[\"input_lens\"]\n",
    "\n",
    "    @property\n",
    "    def output_len(self) -> list[int]:\n",
    "        return self.vllm_metrics[\"output_lens\"]\n",
    "\n",
    "    @property\n",
    "    def max_concurrency(self) -> int:\n",
    "        return self.vllm_metrics[\"max_concurrency\"]\n",
    "\n",
    "    @property\n",
    "    def request_rate(self) -> str:\n",
    "        return self.vllm_metrics[\"request_rate\"]\n",
    "\n",
    "    @property\n",
    "    def burstiness(self) -> float:\n",
    "        return self.vllm_metrics[\"burstiness\"]\n",
    "\n",
    "    # Output\n",
    "\n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return self.vllm_metrics[\"duration\"]\n",
    "\n",
    "    @property\n",
    "    def max_output_tokens_per_second(self) -> float:\n",
    "        return self.vllm_metrics[\"max_output_tokens_per_s\"]\n",
    "\n",
    "    @property\n",
    "    def max_concurrent_requests(self) -> int:\n",
    "        return self.vllm_metrics[\"max_concurrent_requests\"]\n",
    "\n",
    "    def get_vllm_key(self, key: str):\n",
    "        return self.vllm_metrics[key]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"[{self.index}] {self.model_id}, num prompts: {self.num_prompts}, input length: {self.input_len[0]}, output length: {self.output_len[0]}, concurrency: {self.max_concurrent_requests}/{self.max_concurrency}, request rate: {self.request_rate}, burstiness: {self.burstiness}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295f99c",
   "metadata": {},
   "source": [
    "## 0. Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent folder containing the data\n",
    "data_path = Path(\"data\", \"calibration\")  # ← FIXME\n",
    "# The experiments\n",
    "exp_throughput_latency = Experiment(data_path.joinpath(\"throughput_latency\"))\n",
    "sweep_1   = Experiment(data_path.joinpath(\"saturation_point\"))\n",
    "exp_sequence_overhead  = Experiment(data_path.joinpath(\"sequence_overhead\"))\n",
    "exp_energy             = Experiment(data_path.joinpath(\"energy\"))\n",
    "# All experiments\n",
    "all_exps = (exp_throughput_latency, sweep_1, exp_sequence_overhead, exp_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_seconds_long(seconds: float) -> str:\n",
    "    h, remainder = divmod(int(seconds), 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "nb_total_runs = 0\n",
    "duration_total = 0\n",
    "print(f\"• Number of experiments: {len(all_exps)}\")\n",
    "for exp in all_exps:\n",
    "    print(f\"  • {str(exp)}\")\n",
    "    all_runs = exp.get_all_runs()\n",
    "    nb_total_runs += len(all_runs)\n",
    "    for run in all_runs:\n",
    "        duration_total += run.get_vllm_key(\"duration\")\n",
    "print(f\"• Total measurements: {nb_total_runs}\")\n",
    "print(f\"• Total duration: {format_seconds_long(duration_total)}\")\n",
    "print(f\"• Estimated Azure price: {round(duration_total / 3600 * 7, 3)} €\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93012090",
   "metadata": {},
   "source": [
    "## 1. Throughput and Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac2bf2",
   "metadata": {},
   "source": [
    "### 1.1. Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ef16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_throughput_latency = Experiment(data_path.joinpath(\"throughput_latency\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc172c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []  # We build row by row\n",
    "for c in exp_throughput_latency.conditions:\n",
    "    for run in c.runs:\n",
    "        assert all(e == \"\" for e in run.get_vllm_key(\"errors\")), (\n",
    "            f\"vLLM reported an error during measurement. Check .json {run.path_vllm_json}\"\n",
    "        )\n",
    "        assert run.get_vllm_key(\"completed\") == run.get_vllm_key(\"num_prompts\"), \"Run crashed.\"\n",
    "        rows.append(\n",
    "            {\n",
    "                # Condition\n",
    "                \"condition\": c.name,\n",
    "                \"tee_mode\": c.tee_mode.value,\n",
    "                # Measurement\n",
    "                \"Measurement #\": run.index,\n",
    "                \"duration (s)\": round(run.get_vllm_key(\"duration\")),\n",
    "                # Throughput\n",
    "                \"output throughput (tok/s)\": run.get_vllm_key(\"output_throughput\"),\n",
    "                \"total token throughput (tok/s)\": run.get_vllm_key(\n",
    "                    \"total_token_throughput\"\n",
    "                ),\n",
    "                # Latency TODO\n",
    "                # Cloud\n",
    "                \"Azure cost (€)\": round(run.get_vllm_key(\"duration\") / 3600 * 7, 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb922dee",
   "metadata": {},
   "source": [
    "### (GPU calibaration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c805b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_mns_calibration = Experiment(data_path.joinpath(\"throughput_latency_mns-calibration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a85a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns for plotting\n",
    "def pre_process_gpu_metrics(run: Run) -> pd.DataFrame:\n",
    "    MAX_WATTS = 700\n",
    "    gpu_metrics = run.gpu_metrics\n",
    "    gpu_metrics[\"power_draw_watts\"] = gpu_metrics[\" power.draw [W]\"].str.rstrip(\"W\").str.strip().astype(float)\n",
    "    gpu_metrics[\"power_draw_percent\"] = gpu_metrics[\"power_draw_watts\"] / MAX_WATTS * 100\n",
    "    gpu_metrics[\"utilization_gpu_percent\"] = gpu_metrics[\" utilization.gpu [%]\"].str.rstrip(\"%\").str.strip().astype(float)\n",
    "    gpu_metrics[\"utilization_memory_percent\"] = gpu_metrics[\" utilization.memory [%]\"].str.rstrip(\"%\").str.strip().astype(float)\n",
    "    gpu_metrics[\"temperature_gpu_celsius\"] = gpu_metrics[\" temperature.gpu\"]\n",
    "    return gpu_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpu_metrics_for_condition(condition: Condition):\n",
    "    measurements = condition.get_all_runs()\n",
    "    n_measurements = len(measurements)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_cols = min(3, n_measurements)  # Max 3 columns\n",
    "    n_rows = (n_measurements + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    \n",
    "    # Flatten axes array for easier iteration\n",
    "    if n_measurements == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, measurement in enumerate(measurements):\n",
    "        ax = axes[idx]\n",
    "        gpu_metrics = pre_process_gpu_metrics(measurement)\n",
    "        \n",
    "        # Plot metrics\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"power_draw_percent\"], linewidth=1.5, alpha=0.8, label=\"Power Draw (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"utilization_gpu_percent\"], linewidth=1.5, alpha=0.8, label=\"GPU Utilization (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"utilization_memory_percent\"], linewidth=1.5, alpha=0.8, label=\"Memory Utilization (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"temperature_gpu_celsius\"], linewidth=1.5, alpha=0.8, label=\"Temperature (°C)\")\n",
    "        \n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_yticks(range(0, 101, 10))\n",
    "        ax.set_xlabel(\"Sample Index\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.set_title(f\"GPU Usage Over Time - Measurement {measurement.index}\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(measurements), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    # Add main title\n",
    "    fig.suptitle(f\"GPU Metrics: {condition.name} ({condition.tee_mode.value})\", fontsize=14, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exp_mns_calibration.get_all_runs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a35a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_512 = exp_mns_calibration.get_condition(\"Gemma3 1B-mns-512\", TEE_Mode.TEE_ON)\n",
    "gemma_1024 = exp_mns_calibration.get_condition(\"Gemma3 1B-mns-1024\", TEE_Mode.TEE_ON)\n",
    "llama_256 = exp_mns_calibration.get_condition(\"Llama3.1 8B-mns-256\", TEE_Mode.TEE_ON)\n",
    "mistral_128 = exp_mns_calibration.get_condition(\"Mistral 24B-mns-128\", TEE_Mode.TEE_ON)\n",
    "mistral_32 = exp_mns_calibration.get_condition(\"Mistral 24B-mns-32\", TEE_Mode.TEE_ON)\n",
    "qwen_64 = exp_mns_calibration.get_condition(\"Qwen3 32B-mns-64\", TEE_Mode.TEE_ON)\n",
    "qwen_16 = exp_mns_calibration.get_condition(\"Qwen3 32B-mns-16\", TEE_Mode.TEE_ON)\n",
    "\n",
    "\n",
    "plot_gpu_metrics_for_condition(gemma_512)\n",
    "plot_gpu_metrics_for_condition(gemma_1024)\n",
    "plot_gpu_metrics_for_condition(llama_256)\n",
    "plot_gpu_metrics_for_condition(mistral_128)\n",
    "plot_gpu_metrics_for_condition(mistral_32)\n",
    "plot_gpu_metrics_for_condition(qwen_64)\n",
    "plot_gpu_metrics_for_condition(qwen_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get run durations per prompt\n",
    "\n",
    "def get_run_durations(condition: Condition):\n",
    "    return [run.get_vllm_key(\"duration\") / run.get_vllm_key(\"num_prompts\") for run in condition.get_all_runs()]\n",
    "\n",
    "print(f\"gemma_512: {get_run_durations(gemma_512)}\") # ← You\n",
    "print(f\"gemma_1024: {get_run_durations(gemma_1024)}\")\n",
    "print(f\"llama_256: {get_run_durations(llama_256)}\") # ← You\n",
    "print(f\"mistral_128: {get_run_durations(mistral_128)}\") # ← You\n",
    "print(f\"mistral_32: {get_run_durations(mistral_32)}\")\n",
    "print(f\"qwen_64: {get_run_durations(qwen_64)}\") # ← You\n",
    "print(f\"qwen_16: {get_run_durations(qwen_16)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a648b",
   "metadata": {},
   "source": [
    "### 1.2. Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_labels = []\n",
    "throughput_medians_tee_on, tee_on_stds = [], []\n",
    "throughput_medians_tee_off, tee_off_stds = [], []\n",
    "\n",
    "for c in exp_throughput_latency.list_condition_names(sorting=Sorting.MODEL_SIZE):\n",
    "    condition_labels.append(c)\n",
    "    \n",
    "    # TEE ON\n",
    "    median_tee_on, std_tee_on = exp_throughput_latency.get_condition(\n",
    "        c, TEE_Mode.TEE_ON\n",
    "    ).get_median_throughput_with_std()\n",
    "    throughput_medians_tee_on.append(median_tee_on)\n",
    "    tee_on_stds.append(std_tee_on)\n",
    "    # TEE OFF\n",
    "    median_tee_off, std_tee_off = exp_throughput_latency.get_condition(\n",
    "        c, TEE_Mode.TEE_OFF\n",
    "    ).get_median_throughput_with_std()\n",
    "    throughput_medians_tee_off.append(median_tee_off)\n",
    "    tee_off_stds.append(std_tee_off)\n",
    "\n",
    "x = np.arange(len(condition_labels))\n",
    "width = 0.20  # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(\n",
    "    x - width / 2,\n",
    "    throughput_medians_tee_off,\n",
    "    width,\n",
    "    yerr=tee_off_stds,\n",
    "    label=\"TEE off\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax.bar(\n",
    "    x + width / 2,\n",
    "    throughput_medians_tee_on,\n",
    "    width,\n",
    "    yerr=tee_on_stds,\n",
    "    label=\"TEE On\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add labels, title and legend\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Output throughput (tok/s)\")\n",
    "ax.set_title(\"Throughput Comparison: TEE Off vs TEE On\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(condition_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782e86c",
   "metadata": {},
   "source": [
    "### 1.3. Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53459f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttft_latency_metrics(condition: Condition):\n",
    "    runs = condition.get_all_runs()\n",
    "    run_medians = []\n",
    "    run_p95s = []\n",
    "    for run in runs:\n",
    "        ttfts = run.get_vllm_key(\"ttfts\")\n",
    "        run_medians.append(np.median(ttfts))\n",
    "        run_p95s.append(np.percentile(ttfts, 95))\n",
    "    median_ttft = np.median(run_medians)\n",
    "    std_ttft = np.std(run_medians, ddof=1)\n",
    "    p95_ttft = np.median(run_p95s)\n",
    "    std_p95 = np.std(run_p95s, ddof=1)\n",
    "    return median_ttft, std_ttft, p95_ttft, std_p95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5291986",
   "metadata": {},
   "source": [
    "#### 1.3.1. Prefill: Time to First Token (TTFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "medians = []\n",
    "stds = []\n",
    "p95s = []\n",
    "std_p95s = []\n",
    "\n",
    "for c in exp_throughput_latency.conditions:\n",
    "    models.append(c.model_name)\n",
    "    median, std, p95, std_p95 = get_ttft_latency_metrics(c)\n",
    "    medians.append(median)\n",
    "    stds.append(std)\n",
    "    p95s.append(p95)\n",
    "    std_p95s.append(std_p95)\n",
    "\n",
    "condition_labels = []\n",
    "latency_medians_tee_on, tee_on_stds = [], []\n",
    "latency_medians_tee_off, tee_off_stds = [], []\n",
    "p95_latencies_tee_on = []\n",
    "p95_latencies_tee_off = []\n",
    "\n",
    "for c in exp_throughput_latency.list_condition_names(sorting=Sorting.MODEL_SIZE):\n",
    "    condition_labels.append(c)\n",
    "    \n",
    "    # TEE ON\n",
    "    median_tee_on, std_tee_on, p95_tee_on, std_p95_tee_on = get_ttft_latency_metrics(\n",
    "        exp_throughput_latency.get_condition(c, TEE_Mode.TEE_ON)\n",
    "    )\n",
    "    latency_medians_tee_on.append(median_tee_on)\n",
    "    tee_on_stds.append(std_tee_on)\n",
    "    p95_latencies_tee_on.append(p95_tee_on)\n",
    "    \n",
    "    # TEE OFF\n",
    "    median_tee_off, std_tee_off, p95_tee_off, std_p95_tee_off = get_ttft_latency_metrics(\n",
    "        exp_throughput_latency.get_condition(c, TEE_Mode.TEE_OFF)\n",
    "    )\n",
    "    latency_medians_tee_off.append(median_tee_off)\n",
    "    tee_off_stds.append(std_tee_off)\n",
    "    p95_latencies_tee_off.append(p95_tee_off)\n",
    "\n",
    "y = np.arange(len(condition_labels))\n",
    "height = 0.20  # Height of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.barh(\n",
    "    y - height / 2,\n",
    "    latency_medians_tee_off,\n",
    "    height,\n",
    "    xerr=tee_off_stds,\n",
    "    label=\"TEE off\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax.barh(\n",
    "    y + height / 2,\n",
    "    latency_medians_tee_on,\n",
    "    height,\n",
    "    xerr=tee_on_stds,\n",
    "    label=\"TEE On\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add whiskers for p95 latency (drawn as thin lines extending rightward)\n",
    "for i, (median_off, p95_off) in enumerate(zip(latency_medians_tee_off, p95_latencies_tee_off)):\n",
    "    ax.plot([median_off, p95_off], [i - height / 2, i - height / 2], color=\"black\", lw=1.5)\n",
    "    ax.scatter(p95_off, i - height / 2, color=\"black\", s=20, zorder=3)\n",
    "\n",
    "for i, (median_on, p95_on) in enumerate(zip(latency_medians_tee_on, p95_latencies_tee_on)):\n",
    "    ax.plot([median_on, p95_on], [i + height / 2, i + height / 2], color=\"black\", lw=1.5)\n",
    "    ax.scatter(p95_on, i + height / 2, color=\"black\", s=20, zorder=3)\n",
    "\n",
    "# Add labels, title and legend\n",
    "ax.set_xlabel(\"Time to First Token (ms)\")\n",
    "ax.set_ylabel(\"Model\")\n",
    "ax.set_title(\"Latency Comparison: TEE Off vs TEE On\")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(condition_labels)\n",
    "ax.legend()\n",
    "#ax.invert_yaxis()  # highest on top\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = exp_throughput_latency.get_condition(\"Gemma3 1B\", TEE_Mode.TEE_ON)\n",
    "\n",
    "\n",
    "def get_decode_latency_metrics(condition: Condition):\n",
    "    runs = condition.get_all_runs()\n",
    "    run_medians = []\n",
    "    run_p95s = []\n",
    "\n",
    "    for run in runs:\n",
    "        # Fetch aggregated TPOT stats\n",
    "        median_tpot = run.get_vllm_key(\"median_tpot_ms\")\n",
    "        p95_tpot = run.get_vllm_key(\"p95_tpot_ms\")\n",
    "\n",
    "        # Compute mean output length for this run\n",
    "        output_lens = np.array(run.get_vllm_key(\"output_lens\"))\n",
    "        mean_output_len = np.mean(output_lens)\n",
    "\n",
    "        # Estimate total decode latency for this run (ms)\n",
    "        run_medians.append(median_tpot * mean_output_len)\n",
    "        run_p95s.append(p95_tpot * mean_output_len)\n",
    "\n",
    "    # Aggregate across runs\n",
    "    median_decode = np.median(run_medians)\n",
    "    std_decode = np.std(run_medians, ddof=1)\n",
    "    p95_decode = np.median(run_p95s)\n",
    "    std_p95 = np.std(run_p95s, ddof=1)\n",
    "\n",
    "    return median_decode, std_decode, p95_decode, std_p95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ab96b",
   "metadata": {},
   "source": [
    "#### 1.3.2. Decode latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_labels_decode = []\n",
    "decode_latency_medians_tee_on, decode_tee_on_stds = [], []\n",
    "decode_latency_medians_tee_off, decode_tee_off_stds = [], []\n",
    "p95_decode_latencies_tee_on = []\n",
    "p95_decode_latencies_tee_off = []\n",
    "\n",
    "for c in exp_throughput_latency.list_condition_names(sorting=Sorting.MODEL_SIZE):\n",
    "    condition_labels_decode.append(c)\n",
    "    \n",
    "    # TEE ON\n",
    "    median_tee_on, std_tee_on, p95_tee_on, std_p95_tee_on = get_decode_latency_metrics(\n",
    "        exp_throughput_latency.get_condition(c, TEE_Mode.TEE_ON)\n",
    "    )\n",
    "    decode_latency_medians_tee_on.append(median_tee_on)\n",
    "    decode_tee_on_stds.append(std_tee_on)\n",
    "    p95_decode_latencies_tee_on.append(p95_tee_on)\n",
    "    \n",
    "    # TEE OFF\n",
    "    median_tee_off, std_tee_off, p95_tee_off, std_p95_tee_off = get_decode_latency_metrics(\n",
    "        exp_throughput_latency.get_condition(c, TEE_Mode.TEE_OFF)\n",
    "    )\n",
    "    decode_latency_medians_tee_off.append(median_tee_off)\n",
    "    decode_tee_off_stds.append(std_tee_off)\n",
    "    p95_decode_latencies_tee_off.append(p95_tee_off)\n",
    "\n",
    "y = np.arange(len(condition_labels_decode))\n",
    "height = 0.20  # Height of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.barh(\n",
    "    y - height / 2,\n",
    "    decode_latency_medians_tee_off,\n",
    "    height,\n",
    "    xerr=decode_tee_off_stds,\n",
    "    label=\"TEE off\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax.barh(\n",
    "    y + height / 2,\n",
    "    decode_latency_medians_tee_on,\n",
    "    height,\n",
    "    xerr=decode_tee_on_stds,\n",
    "    label=\"TEE On\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add whiskers for p95 latency (drawn as thin lines extending rightward)\n",
    "for i, (median_off, p95_off) in enumerate(zip(decode_latency_medians_tee_off, p95_decode_latencies_tee_off)):\n",
    "    ax.plot([median_off, p95_off], [i - height / 2, i - height / 2], color=\"black\", lw=1.5)\n",
    "    ax.scatter(p95_off, i - height / 2, color=\"black\", s=20, zorder=3)\n",
    "\n",
    "for i, (median_on, p95_on) in enumerate(zip(decode_latency_medians_tee_on, p95_decode_latencies_tee_on)):\n",
    "    ax.plot([median_on, p95_on], [i + height / 2, i + height / 2], color=\"black\", lw=1.5)\n",
    "    ax.scatter(p95_on, i + height / 2, color=\"black\", s=20, zorder=3)\n",
    "\n",
    "# Add labels, title and legend\n",
    "ax.set_xlabel(\"Decode Latency (ms)\")\n",
    "ax.set_ylabel(\"Model\")\n",
    "ax.set_title(\"Decode Latency Comparison: TEE Off vs TEE On\")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(condition_labels_decode)\n",
    "ax.legend()\n",
    "#ax.invert_yaxis()  # highest on top\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0b05a",
   "metadata": {},
   "source": [
    "## 2. Saturation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saturation_metrics(exp_saturation_point):\n",
    "\n",
    "    x_qps = []\n",
    "    y_qps_tee_on_median = []\n",
    "    y_qps_tee_on_std = []\n",
    "    y_qps_tee_off_median = []\n",
    "    y_qps_tee_off_std = []\n",
    "    y_p95_tee_on = []\n",
    "    y_p95_tee_off = []\n",
    "\n",
    "    for condition_name in exp_saturation_point.list_condition_names(Sorting.NATURAL):\n",
    "        # X axis\n",
    "        x_qps.append(int(condition_name.split(\"_\")[-1]))\n",
    "        # Y TEE_ON\n",
    "        runs_TEE_On = exp_saturation_point.get_condition(condition_name, TEE_Mode.TEE_ON).get_all_runs()\n",
    "        run_QPSs_On = [run.get_vllm_key(\"max_concurrent_requests\") for run in runs_TEE_On]\n",
    "        run_p95s_On = [run.get_vllm_key(\"p95_e2el_ms\") for run in runs_TEE_On]\n",
    "        y_qps_tee_on_median.append(np.median(run_QPSs_On))\n",
    "        y_qps_tee_on_std.append(np.std(run_QPSs_On))\n",
    "        y_p95_tee_on.append(np.median(run_p95s_On))\n",
    "        # Y TEE_OFF\n",
    "        runs_TEE_Off = exp_saturation_point.get_condition(condition_name, TEE_Mode.TEE_OFF).get_all_runs()\n",
    "        run_QPSs_Off = [run.get_vllm_key(\"max_concurrent_requests\") for run in runs_TEE_Off]\n",
    "        y_qps_tee_off_median.append(np.median(run_QPSs_Off))\n",
    "        y_qps_tee_off_std.append(np.std(run_QPSs_Off))\n",
    "        run_p95s_Off = [run.get_vllm_key(\"p95_e2el_ms\") for run in runs_TEE_Off]\n",
    "        y_p95_tee_off.append(np.median(run_p95s_Off))\n",
    "    \n",
    "    return {\n",
    "        \"x_qps\": x_qps,\n",
    "        \"y_qps_tee_on_median\": y_qps_tee_on_median,\n",
    "        \"y_qps_tee_on_std\": y_qps_tee_on_std,\n",
    "        \"y_qps_tee_off_median\": y_qps_tee_off_median,\n",
    "        \"y_qps_tee_off_std\": y_qps_tee_off_std,\n",
    "        \"y_p95_tee_on\": y_p95_tee_on,\n",
    "        \"y_p95_tee_off\": y_p95_tee_off,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_saturation_metrics(metrics: dict):\n",
    "\n",
    "    x_qps = metrics[\"x_qps\"]\n",
    "    y_qps_tee_on_median = metrics[\"y_qps_tee_on_median\"]\n",
    "    y_qps_tee_on_std = metrics[\"y_qps_tee_on_std\"]\n",
    "    y_qps_tee_off_median = metrics[\"y_qps_tee_off_median\"]\n",
    "    y_qps_tee_off_std = metrics[\"y_qps_tee_off_std\"]\n",
    "    y_p95_tee_on = metrics[\"y_p95_tee_on\"]\n",
    "    y_p95_tee_off = metrics[\"y_p95_tee_off\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot TEE ON with error bars on primary Y-axis\n",
    "    ax.errorbar(\n",
    "        x_qps, \n",
    "        y_qps_tee_on_median, \n",
    "        yerr=y_qps_tee_on_std,\n",
    "        label=\"TEE On (Median QPS)\",\n",
    "        #marker=\"o\",\n",
    "        capsize=5,\n",
    "        capthick=2,\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "        alpha=0.8,\n",
    "        elinewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Plot TEE OFF with error bars on primary Y-axis\n",
    "    # ax.errorbar(\n",
    "    #     x_qps, \n",
    "    #     y_qps_tee_off_median, \n",
    "    #     yerr=y_qps_tee_off_std,\n",
    "    #     label=\"TEE Off (Median QPS)\",\n",
    "    #     #marker=\"s\",\n",
    "    #     capsize=5,\n",
    "    #     capthick=2,\n",
    "    #     linewidth=2,\n",
    "    #     markersize=8,\n",
    "    #     alpha=0.8,\n",
    "    #     elinewidth=1.5\n",
    "    # )\n",
    "\n",
    "    ax.set_xlabel(\"Requested Concurrency\")\n",
    "    ax.set_ylabel(\"Achieved QPS\")\n",
    "    ax.tick_params(axis='y')\n",
    "\n",
    "    # Create secondary Y-axis for P95 values\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    # Plot P95 TEE ON on secondary Y-axis\n",
    "    ax2.plot(\n",
    "        x_qps, \n",
    "        y_p95_tee_on,\n",
    "        label=\"TEE On (P95)\",\n",
    "        #marker=\"o\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        markersize=1,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "    # Plot P95 TEE OFF on secondary Y-axis\n",
    "    # ax2.plot(\n",
    "    #     x_qps, \n",
    "    #     y_p95_tee_off,\n",
    "    #     label=\"TEE Off (P95)\",\n",
    "    #     #marker=\"s\",\n",
    "    #     linestyle=\"--\",\n",
    "    #     linewidth=1,\n",
    "    #     markersize=1,\n",
    "    #     alpha=0.6,\n",
    "    # )\n",
    "\n",
    "    ax2.set_ylabel(\"P95 Concurrent Requests\")\n",
    "    ax2.tick_params(axis='y')\n",
    "\n",
    "    ax.set_title(\"Saturation Point: TEE On vs TEE Off (Median + P95)\")\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4325d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_1 = Experiment(data_path.joinpath(\"saturation_point_sweep_1\"))\n",
    "metrics = get_saturation_metrics(sweep_1)\n",
    "plot_saturation_metrics(metrics)\n",
    "for run in sweep_1.get_all_runs():\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fdad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_2 = Experiment(data_path.joinpath(\"saturation_point_sweep_2\"))\n",
    "metrics = get_saturation_metrics(sweep_2)\n",
    "plot_saturation_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = sweep_1.get_all_runs()\n",
    "for run in runs:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = sweep_2.get_all_runs()\n",
    "\n",
    "duration = 0.0\n",
    "for run in runs:\n",
    "    duration += run.get_vllm_key(\"duration\")\n",
    "\n",
    "hours = int(duration // 3600)\n",
    "minutes = int((duration % 3600) // 60)\n",
    "seconds = int(duration % 60)\n",
    "\n",
    "print(f\"{hours}:{minutes}:{seconds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ef0d4",
   "metadata": {},
   "source": [
    "## 3. Sequence length overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_sequence_overhead = Experiment(data_path.joinpath(\"sequence_overhead\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb30084",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = []\n",
    "output_lengths = []\n",
    "tee_on_throughputs_medians = []\n",
    "tee_on_throughputs_stds = []\n",
    "tee_off_throughputs_medians = []\n",
    "tee_off_throughputs_stds = []\n",
    "\n",
    "\n",
    "for condition in exp_sequence_overhead.list_condition_names(Sorting.NATURAL):\n",
    "    input_len = int(condition.split(\"_\")[1])\n",
    "    output_len = int(condition.split(\"_\")[-1])\n",
    "    input_lengths.append(input_len)\n",
    "    output_lengths.append(output_len)\n",
    "    # TEE_ON\n",
    "    tee_on_throughputs = [r.get_vllm_key(\"total_token_throughput\") for r in exp_sequence_overhead.get_condition(condition, TEE_Mode.TEE_ON).get_all_runs()]\n",
    "    tee_on_throughputs_median = np.median(tee_on_throughputs)\n",
    "    tee_on_throughputs_std = np.std(tee_on_throughputs)\n",
    "    tee_on_throughputs_medians.append(tee_on_throughputs_median)\n",
    "    tee_on_throughputs_stds.append(tee_on_throughputs_std)\n",
    "    # TEE_OFF\n",
    "    tee_off_throughputs = [r.get_vllm_key(\"total_token_throughput\") for r in exp_sequence_overhead.get_condition(condition, TEE_Mode.TEE_OFF).get_all_runs()]\n",
    "    tee_off_throughputs_median = np.median(tee_off_throughputs)\n",
    "    tee_off_throughputs_std = np.std(tee_off_throughputs)\n",
    "    tee_off_throughputs_medians.append(tee_off_throughputs_median)\n",
    "    tee_off_throughputs_stds.append(tee_off_throughputs_std)\n",
    "\n",
    "    print(f\"Input length: {input_len}, Output length: {output_len}\")\n",
    "    print(f\"TEE On throughputs: {tee_on_throughputs}\")\n",
    "    print(f\"TEE Off throughputs: {tee_off_throughputs}\")\n",
    "    print(f\"Overhead: {(tee_off_throughputs_median - tee_on_throughputs_median) / tee_on_throughputs_median * 100}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap using the already-processed data\n",
    "# Build a dictionary from the lists: (input_len, output_len) -> throughput\n",
    "heatmap_data = {}\n",
    "unique_input_lengths = sorted(set(input_lengths))\n",
    "unique_output_lengths = sorted(set(output_lengths))\n",
    "\n",
    "for in_len, out_len, throughput in zip(input_lengths, output_lengths, tee_on_throughputs_medians):\n",
    "    heatmap_data[(in_len, out_len)] = throughput\n",
    "\n",
    "# Create pivot table for heatmap (rows = output lengths, columns = input lengths)\n",
    "heatmap_matrix = np.zeros((len(unique_output_lengths), len(unique_input_lengths)))\n",
    "\n",
    "for i, out_len in enumerate(unique_output_lengths):\n",
    "    for j, in_len in enumerate(unique_input_lengths):\n",
    "        if (in_len, out_len) in heatmap_data:\n",
    "            heatmap_matrix[i, j] = heatmap_data[(in_len, out_len)]\n",
    "        else:\n",
    "            heatmap_matrix[i, j] = np.nan\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "im = ax.imshow(heatmap_matrix, cmap='viridis', aspect='auto', interpolation='nearest', origin='lower')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(unique_input_lengths)))\n",
    "ax.set_yticks(np.arange(len(unique_output_lengths)))\n",
    "ax.set_xticklabels(unique_input_lengths)\n",
    "ax.set_yticklabels(unique_output_lengths)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Input Length (tokens)\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Output Length (tokens)\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Median Total Token Throughput (TEE ON) - Sequence Overhead\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Total Token Throughput (tok/s)\", fontsize=11)\n",
    "\n",
    "# Add values in cells\n",
    "for i in range(len(unique_output_lengths)):\n",
    "    for j in range(len(unique_input_lengths)):\n",
    "        if not np.isnan(heatmap_matrix[i, j]):\n",
    "            text = ax.text(j, i, f\"{heatmap_matrix[i, j]:.1f}\",\n",
    "                          ha=\"center\", va=\"center\", color=\"white\" if heatmap_matrix[i, j] < np.nanmax(heatmap_matrix)/2 else \"black\",\n",
    "                          fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a576b95",
   "metadata": {},
   "source": [
    "## 4. Energy efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd73a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_energy = Experiment(data_path.joinpath(\"energy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_J_per_token(run: Run):\n",
    "    gpu_metrics = pre_process_gpu_metrics(run)\n",
    "    power_watts = gpu_metrics['power_draw_watts'].values\n",
    "    # Each row is 1 second, so use dx=1 for uniform integration\n",
    "    energy_joules = np.trapezoid(power_watts, dx=1)\n",
    "    J_per_token = energy_joules / run.get_vllm_key(\"total_output_tokens\")\n",
    "    return J_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = exp_energy.list_condition_names(Sorting.NATURAL)\n",
    "J_per_token_medians_tee_on = []\n",
    "J_per_token_medians_tee_off = []\n",
    "J_per_token_stds_tee_on = []\n",
    "J_per_token_stds_tee_off = []\n",
    "\n",
    "for model in models:\n",
    "    # TEE_ON\n",
    "    J_per_token_tee_on = []\n",
    "    for run in exp_energy.get_condition(model, TEE_Mode.TEE_ON).get_all_runs():\n",
    "        J_per_token_tee_on.append(get_J_per_token(run))\n",
    "    J_per_token_medians_tee_on.append(np.median(J_per_token_tee_on))\n",
    "    J_per_token_stds_tee_on.append(np.std(J_per_token_tee_on))\n",
    "    # TEE_OFF\n",
    "    J_per_token_tee_off = []\n",
    "    for run in exp_energy.get_condition(model, TEE_Mode.TEE_OFF).get_all_runs():\n",
    "        J_per_token_tee_off.append(get_J_per_token(run))\n",
    "    J_per_token_medians_tee_off.append(np.median(J_per_token_tee_off))\n",
    "    J_per_token_stds_tee_off.append(np.std(J_per_token_tee_off))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a54f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot TEE_ON line with error bars\n",
    "ax.errorbar(range(len(models)), J_per_token_medians_tee_on, \n",
    "            yerr=J_per_token_stds_tee_on, \n",
    "            marker='o', label='TEE_ON', linewidth=2, capsize=5, markersize=8)\n",
    "\n",
    "# Plot TEE_OFF line with error bars\n",
    "ax.errorbar(range(len(models)), J_per_token_medians_tee_off, \n",
    "            yerr=J_per_token_stds_tee_off, \n",
    "            marker='s', label='TEE_OFF', linewidth=2, capsize=5, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Energy per Token (J/token)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('GPU Energy Consumption: TEE ON vs TEE OFF', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(models)))\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend(fontsize=11, loc='best')\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Scale X by the jump in model size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f155368",
   "metadata": {},
   "source": [
    "## 4. Price of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8503ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530add0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

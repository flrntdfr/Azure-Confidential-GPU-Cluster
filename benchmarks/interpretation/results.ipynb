{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d75d0ce",
   "metadata": {},
   "source": [
    "<!-- du4://thèse/cai/results.ipynb?d=20251024?loc=ttum?hPa=1020 -->\n",
    "\n",
    "# Confidential Artificial Intelligence: What's the Catch?\n",
    "### _Performance and costs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from functools import cached_property\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\", context=\"paper\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEE_Mode(Enum):\n",
    "    TEE_ON = \"tee_on\"\n",
    "    TEE_OFF = \"tee_off\"\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    path: Path\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.path.stem\n",
    "\n",
    "    @cached_property\n",
    "    def conditions(self) -> List[\"Condition\"]:\n",
    "        return [\n",
    "            Condition(\n",
    "                q, \n",
    "                q.parent.name, \n",
    "                TEE_Mode.TEE_ON if q.name == \"tee_on\" else TEE_Mode.TEE_OFF\n",
    "            )\n",
    "            for q in self.path.glob(\"*/*\")\n",
    "            if q.is_dir() and q.name in {\"tee_on\", \"tee_off\"}\n",
    "        ]\n",
    "\n",
    "    def get_all_conditions_names(self, sort_by_model_size: bool=False):\n",
    "        all_conditions = self.conditions\n",
    "        if sort_by_model_size:\n",
    "            all_conditions_sorted = sorted(all_conditions, key=lambda c: int(c.model_size))\n",
    "            return list(dict.fromkeys(c.name for c in all_conditions_sorted))\n",
    "        else:\n",
    "            return list(dict.fromkeys(c.name for c in all_conditions))\n",
    "\n",
    "    def get_conditions(self, tee_mode: TEE_Mode):\n",
    "        return [c for c in self.conditions if c.tee_mode == tee_mode]\n",
    "\n",
    "    def get_condition(self, name: str, tee_mode: TEE_Mode):\n",
    "        return next(\n",
    "            filter(lambda c: c.name == name and c.tee_mode == tee_mode, self.conditions),\n",
    "            None,\n",
    "        )\n",
    "    \n",
    "    def get_all_runs(self):\n",
    "        return [r for c in self.conditions for r in c.runs]\n",
    "    \n",
    "    def get_runs(self, tee_mode: TEE_Mode):\n",
    "        return [r for c in self.conditions for r in c.runs if c.tee_mode == tee_mode]\n",
    "\n",
    "    def __str__(self):\n",
    "        name = self.name\n",
    "        nb_conditions = len(self.conditions)\n",
    "        nb_total_runs = len(self.get_all_runs())\n",
    "        return f\"Experiment: {name}, Conditions: {nb_conditions} ({nb_total_runs} total measurements)\"\n",
    "\n",
    "@dataclass\n",
    "class Condition:\n",
    "    path: Path\n",
    "    name: str\n",
    "    tee_mode: TEE_Mode\n",
    "\n",
    "    @property\n",
    "    def model_name(self) :\n",
    "        return self.path.parent.name.split(\"_\")\n",
    "\n",
    "    @property\n",
    "    def model_size(self) -> str:\n",
    "        return re.search(r\"(\\d+)[bB]\", \"_\".join(self.model_name)).group(1)\n",
    "\n",
    "    @cached_property\n",
    "    def runs(self) -> List[\"Run\"]:\n",
    "        run_paths = list(self.path.glob(\"*repetition_*\"))\n",
    "        json_files = sorted([r for r in run_paths if r.suffix == \".json\"])\n",
    "        csv_files = sorted([r for r in run_paths if r.suffix == \".csv\"])\n",
    "\n",
    "        assert len(list(run_paths)) > 0, \"Empty results\"\n",
    "        assert len(json_files) == len(csv_files), f\"Mismatch: {len(json_files)} .json vs. {len(csv_files)} .csv: {run_paths}\"\n",
    "\n",
    "        return [\n",
    "            Run(\n",
    "                idx, json_file, self.path / f\"{json_file.stem}_power_metrics.csv\"\n",
    "            )\n",
    "            for idx, json_file in enumerate(json_files)\n",
    "        ]\n",
    "\n",
    "    def get_all_runs(self) -> List[\"Run\"]:\n",
    "        return self.runs\n",
    "\n",
    "    def get_run(self, index: int) -> \"Run\":\n",
    "        return self.runs[index]\n",
    "\n",
    "    def get_median_throughput_with_std(self) -> Tuple[float, float]:\n",
    "        output_throughputs = [\n",
    "            rep.get_vllm_key(\"output_throughput\") for rep in self.runs\n",
    "        ]\n",
    "        return np.median(output_throughputs), np.std(output_throughputs)\n",
    "    \n",
    "    def get_median_ttft_with_std_and_p95(self):# -> Tuple[float, float, float]:\n",
    "        latencies = [\n",
    "            rep.get_vllm_key(\"ttfts\") for rep in self.runs \n",
    "        ]\n",
    "        #return np.median(latencies), np.std(ltencies), np.percentile(latencies, 95) # TODO: compare to VLLM output\n",
    "        return latencies\n",
    "\n",
    "    def get_median_itl_with_std_and_p95(self) -> Tuple[float, float, float]:\n",
    "        latencies = [\n",
    "            rep.get_vllm_key(\"itls\") for rep in self.runs\n",
    "        ]\n",
    "        return np.median(latencies), np.std(latencies), np.percentile(latencies, 95) # TODO: compare to VLLM output\n",
    "\n",
    "@dataclass\n",
    "class Run:\n",
    "    index: int\n",
    "    path_vllm_json: Path\n",
    "    path_power_csv: Path\n",
    "\n",
    "    @cached_property\n",
    "    def vllm_metrics(self) -> dict:\n",
    "        return json.loads(self.path_vllm_json.read_text())\n",
    "\n",
    "    @cached_property\n",
    "    def gpu_metrics(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.path_power_csv)\n",
    "\n",
    "    @property\n",
    "    def dataset(self) -> str:\n",
    "        return self.vllm_metrics[\"dataset\"]\n",
    "\n",
    "    @property\n",
    "    def model_id(self) -> str:\n",
    "        return self.vllm_metrics[\"model\"]\n",
    "\n",
    "    @property\n",
    "    def input_length(self) -> int:\n",
    "        return self.vllm_metrics[\"input_length\"]\n",
    "\n",
    "    @property\n",
    "    def output_length(self) -> int:\n",
    "        return self.vllm_metrics[\"output_length\"]\n",
    "\n",
    "    @property\n",
    "    def concurrency(self) -> int:\n",
    "        return self.vllm_metrics[\"concurrency\"]\n",
    "\n",
    "    @property\n",
    "    def temperature(self) -> float:\n",
    "        return self.vllm_metrics[\"temperature\"]\n",
    "\n",
    "    def get_vllm_key(self, key: str):\n",
    "        return self.vllm_metrics[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295f99c",
   "metadata": {},
   "source": [
    "## 0. Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent folder containing the data\n",
    "data_path = Path(\"data\", \"calibration\")  # ← FIXME\n",
    "# The experiments\n",
    "exp_throughput_latency = Experiment(data_path.joinpath(\"throughput_latency\"))\n",
    "exp_saturation_point   = Experiment(data_path.joinpath(\"saturation_point\"))\n",
    "exp_sequence_overhead  = Experiment(data_path.joinpath(\"sequence_overhead\"))\n",
    "exp_energy             = Experiment(data_path.joinpath(\"energy\"))\n",
    "# All experiments\n",
    "all_exps = (exp_throughput_latency, exp_saturation_point, exp_sequence_overhead, exp_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_seconds_long(seconds: float) -> str:\n",
    "    h, remainder = divmod(int(seconds), 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "nb_total_runs = 0\n",
    "duration_total = 0\n",
    "print(f\"• Number of experiments: {len(all_exps)}\")\n",
    "for exp in all_exps:\n",
    "    print(f\"  • {str(exp)}\")\n",
    "    all_runs = exp.get_all_runs()\n",
    "    nb_total_runs += len(all_runs)\n",
    "    for m in all_runs:\n",
    "        duration_total += m.get_vllm_key(\"duration\")\n",
    "print(f\"• Total measurements: {nb_total_runs}\")\n",
    "print(f\"• Total duration: {format_seconds_long(duration_total)}\")\n",
    "print(f\"• Estimated Azure price: {round(duration_total / 3600 * 7, 3)} €\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93012090",
   "metadata": {},
   "source": [
    "## 1. Throughput and Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac2bf2",
   "metadata": {},
   "source": [
    "### 1.1. Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc172c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []  # We build row by row\n",
    "for c in exp_throughput_latency.conditions:\n",
    "    for m in c.runs:\n",
    "        assert all(e == \"\" for e in m.get_vllm_key(\"errors\")), (\n",
    "            f\"vLLM reported an error during measurement. Check .json {m.path_vllm_json}\"\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                # Condition\n",
    "                \"condition\": c.name,\n",
    "                \"tee_mode\": c.tee_mode.value,\n",
    "                # Measurement\n",
    "                \"Measurement #\": m.index,\n",
    "                \"duration (s)\": round(m.get_vllm_key(\"duration\")),\n",
    "                # Throughput\n",
    "                \"output throughput (tok/s)\": m.get_vllm_key(\"output_throughput\"),\n",
    "                \"total token throughput (tok/s)\": m.get_vllm_key(\n",
    "                    \"total_token_throughput\"\n",
    "                ),\n",
    "                # Latency TODO\n",
    "                # Cloud\n",
    "                \"Azure cost (€)\": round(m.get_vllm_key(\"duration\") / 3600 * 7, 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a648b",
   "metadata": {},
   "source": [
    "### 1.2. Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_labels = []\n",
    "throughput_medians_tee_on, tee_on_stds = [], []\n",
    "throughput_medians_tee_off, tee_off_stds = [], []\n",
    "\n",
    "for c in exp_throughput_latency.get_all_conditions_names(sort_by_model_size=True):\n",
    "    condition_labels.append(c)\n",
    "    \n",
    "    # TEE ON\n",
    "    median_tee_on, std_tee_on = exp_throughput_latency.get_condition(\n",
    "        c, TEE_Mode.TEE_ON\n",
    "    ).get_median_throughput_with_std()\n",
    "    throughput_medians_tee_on.append(median_tee_on)\n",
    "    tee_on_stds.append(std_tee_on)\n",
    "    # TEE OFF\n",
    "    median_tee_off, std_tee_off = exp_throughput_latency.get_condition(\n",
    "        c, TEE_Mode.TEE_OFF\n",
    "    ).get_median_throughput_with_std()\n",
    "    throughput_medians_tee_off.append(median_tee_off)\n",
    "    tee_off_stds.append(std_tee_off)\n",
    "\n",
    "x = np.arange(len(condition_labels))\n",
    "width = 0.20  # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(\n",
    "    x - width / 2,\n",
    "    throughput_medians_tee_off,\n",
    "    width,\n",
    "    yerr=tee_off_stds,\n",
    "    label=\"TEE off\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax.bar(\n",
    "    x + width / 2,\n",
    "    throughput_medians_tee_on,\n",
    "    width,\n",
    "    yerr=tee_on_stds,\n",
    "    label=\"TEE On\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add labels, title and legend\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Output throughput (tok/s)\")\n",
    "ax.set_title(\"Throughput Comparison: TEE Off vs TEE On\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(condition_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782e86c",
   "metadata": {},
   "source": [
    "### 1.3. Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53459f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttft_latency_metrics(condition: Condition):\n",
    "    runs = condition.get_all_runs()\n",
    "    run_medians = []\n",
    "    run_p95s = []\n",
    "    for run in runs:\n",
    "        ttfts = run.get_vllm_key(\"ttfts\")\n",
    "        run_medians.append(np.median(ttfts))\n",
    "        run_p95s.append(np.percentile(ttfts, 95))\n",
    "    median_ttft = np.median(run_medians)\n",
    "    std_ttft = np.std(run_medians, ddof=1)\n",
    "    p95_ttft = np.median(run_p95s)\n",
    "    std_p95 = np.std(run_p95s, ddof=1)\n",
    "    return median_ttft, std_ttft, p95_ttft, std_p95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5291986",
   "metadata": {},
   "source": [
    "#### 1.3.1. Prefill: Time to First Token (TTFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "medians = []\n",
    "stds = []\n",
    "p95s = []\n",
    "std_p95s = []\n",
    "\n",
    "for c in exp_throughput_latency.conditions:\n",
    "    models.append(c.model_name)\n",
    "    median, std, p95, std_p95 = get_ttft_latency_metrics(c)\n",
    "    medians.append(median)\n",
    "    stds.append(std)\n",
    "    p95s.append(p95)\n",
    "    std_p95s.append(std_p95)\n",
    "\n",
    "condition_labels = []\n",
    "latency_medians_tee_on, tee_on_stds = [], []\n",
    "latency_medians_tee_off, tee_off_stds = [], []\n",
    "p95_latencies_tee_on = []\n",
    "p95_latencies_tee_off = []\n",
    "\n",
    "for c in exp_throughput_latency.get_all_conditions_names(sort_by_model_size=True):\n",
    "    condition_labels.append(c)\n",
    "    \n",
    "    # TEE ON\n",
    "    median_tee_on, std_tee_on, p95_tee_on, std_p95_tee_on = get_ttft_latency_metrics(\n",
    "        exp_throughput_latency.get_condition(c, TEE_Mode.TEE_ON)\n",
    "    )\n",
    "    latency_medians_tee_on.append(median_tee_on)\n",
    "    tee_on_stds.append(std_tee_on)\n",
    "    p95_latencies_tee_on.append(p95_tee_on)\n",
    "    \n",
    "    # TEE OFF\n",
    "    median_tee_off, std_tee_off, p95_tee_off, std_p95_tee_off = get_ttft_latency_metrics(\n",
    "        exp_throughput_latency.get_condition(c, TEE_Mode.TEE_OFF)\n",
    "    )\n",
    "    latency_medians_tee_off.append(median_tee_off)\n",
    "    tee_off_stds.append(std_tee_off)\n",
    "    p95_latencies_tee_off.append(p95_tee_off)\n",
    "\n",
    "y = np.arange(len(condition_labels))\n",
    "height = 0.20  # Height of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.barh(\n",
    "    y - height / 2,\n",
    "    latency_medians_tee_off,\n",
    "    height,\n",
    "    xerr=tee_off_stds,\n",
    "    label=\"TEE off\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax.barh(\n",
    "    y + height / 2,\n",
    "    latency_medians_tee_on,\n",
    "    height,\n",
    "    xerr=tee_on_stds,\n",
    "    label=\"TEE On\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add whiskers for p95 latency (drawn as thin lines extending rightward)\n",
    "for i, (median_off, p95_off) in enumerate(zip(latency_medians_tee_off, p95_latencies_tee_off)):\n",
    "    ax.plot([median_off, p95_off], [i - height / 2, i - height / 2], color=\"black\", lw=1.5)\n",
    "    ax.scatter(p95_off, i - height / 2, color=\"black\", s=20, zorder=3)\n",
    "\n",
    "for i, (median_on, p95_on) in enumerate(zip(latency_medians_tee_on, p95_latencies_tee_on)):\n",
    "    ax.plot([median_on, p95_on], [i + height / 2, i + height / 2], color=\"black\", lw=1.5)\n",
    "    ax.scatter(p95_on, i + height / 2, color=\"black\", s=20, zorder=3)\n",
    "\n",
    "# Add labels, title and legend\n",
    "ax.set_xlabel(\"Time to First Token (ms)\")\n",
    "ax.set_ylabel(\"Model\")\n",
    "ax.set_title(\"Latency Comparison: TEE Off vs TEE On\")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(condition_labels)\n",
    "ax.legend()\n",
    "#ax.invert_yaxis()  # highest on top\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = exp_throughput_latency.get_condition(\"Gemma3 1B\", TEE_Mode.TEE_ON)\n",
    "\n",
    "\n",
    "def get_decode_latency_metrics(condition: Condition):\n",
    "    runs = condition.get_all_runs()\n",
    "    run_medians = []\n",
    "    run_p95s = []\n",
    "\n",
    "    for run in runs:\n",
    "        # Fetch aggregated TPOT stats\n",
    "        median_tpot = run.get_vllm_key(\"median_tpot_ms\")\n",
    "        p95_tpot = run.get_vllm_key(\"p95_tpot_ms\")\n",
    "\n",
    "        # Compute mean output length for this run\n",
    "        output_lens = np.array(run.get_vllm_key(\"output_lens\"))\n",
    "        mean_output_len = np.mean(output_lens)\n",
    "\n",
    "        # Estimate total decode latency for this run (ms)\n",
    "        run_medians.append(median_tpot * mean_output_len)\n",
    "        run_p95s.append(p95_tpot * mean_output_len)\n",
    "\n",
    "    # Aggregate across runs\n",
    "    median_decode = np.median(run_medians)\n",
    "    std_decode = np.std(run_medians, ddof=1)\n",
    "    p95_decode = np.median(run_p95s)\n",
    "    std_p95 = np.std(run_p95s, ddof=1)\n",
    "\n",
    "    return median_decode, std_decode, p95_decode, std_p95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ab96b",
   "metadata": {},
   "source": [
    "#### 1.3.2. Decode latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_labels_decode = []\n",
    "decode_latency_medians_tee_on, decode_tee_on_stds = [], []\n",
    "decode_latency_medians_tee_off, decode_tee_off_stds = [], []\n",
    "p95_decode_latencies_tee_on = []\n",
    "p95_decode_latencies_tee_off = []\n",
    "\n",
    "for c in exp_throughput_latency.get_all_conditions_names(sort_by_model_size=True):\n",
    "    condition_labels_decode.append(c)\n",
    "    \n",
    "    # TEE ON\n",
    "    median_tee_on, std_tee_on, p95_tee_on, std_p95_tee_on = get_decode_latency_metrics(\n",
    "        exp_throughput_latency.get_condition(c, TEE_Mode.TEE_ON)\n",
    "    )\n",
    "    decode_latency_medians_tee_on.append(median_tee_on)\n",
    "    decode_tee_on_stds.append(std_tee_on)\n",
    "    p95_decode_latencies_tee_on.append(p95_tee_on)\n",
    "    \n",
    "    # TEE OFF\n",
    "    median_tee_off, std_tee_off, p95_tee_off, std_p95_tee_off = get_decode_latency_metrics(\n",
    "        exp_throughput_latency.get_condition(c, TEE_Mode.TEE_OFF)\n",
    "    )\n",
    "    decode_latency_medians_tee_off.append(median_tee_off)\n",
    "    decode_tee_off_stds.append(std_tee_off)\n",
    "    p95_decode_latencies_tee_off.append(p95_tee_off)\n",
    "\n",
    "y = np.arange(len(condition_labels_decode))\n",
    "height = 0.20  # Height of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.barh(\n",
    "    y - height / 2,\n",
    "    decode_latency_medians_tee_off,\n",
    "    height,\n",
    "    xerr=decode_tee_off_stds,\n",
    "    label=\"TEE off\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax.barh(\n",
    "    y + height / 2,\n",
    "    decode_latency_medians_tee_on,\n",
    "    height,\n",
    "    xerr=decode_tee_on_stds,\n",
    "    label=\"TEE On\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add whiskers for p95 latency (drawn as thin lines extending rightward)\n",
    "for i, (median_off, p95_off) in enumerate(zip(decode_latency_medians_tee_off, p95_decode_latencies_tee_off)):\n",
    "    ax.plot([median_off, p95_off], [i - height / 2, i - height / 2], color=\"black\", lw=1.5)\n",
    "    ax.scatter(p95_off, i - height / 2, color=\"black\", s=20, zorder=3)\n",
    "\n",
    "for i, (median_on, p95_on) in enumerate(zip(decode_latency_medians_tee_on, p95_decode_latencies_tee_on)):\n",
    "    ax.plot([median_on, p95_on], [i + height / 2, i + height / 2], color=\"black\", lw=1.5)\n",
    "    ax.scatter(p95_on, i + height / 2, color=\"black\", s=20, zorder=3)\n",
    "\n",
    "# Add labels, title and legend\n",
    "ax.set_xlabel(\"Decode Latency (ms)\")\n",
    "ax.set_ylabel(\"Model\")\n",
    "ax.set_title(\"Decode Latency Comparison: TEE Off vs TEE On\")\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(condition_labels_decode)\n",
    "ax.legend()\n",
    "#ax.invert_yaxis()  # highest on top\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0b05a",
   "metadata": {},
   "source": [
    "## 2. Saturation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm_data.get(\"max_concurrent_requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ef0d4",
   "metadata": {},
   "source": [
    "## 3. Sequence length overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a576b95",
   "metadata": {},
   "source": [
    "## 4. Energy efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns for plotting\n",
    "def pre_process_gpu_metrics_for_condition(m: Run) -> pd.DataFrame:\n",
    "    MAX_WATTS = 700\n",
    "    gpu_metrics = m.gpu_metrics\n",
    "    gpu_metrics[\"power_draw_watts\"] = gpu_metrics[\" power.draw [W]\"].str.rstrip(\"W\").str.strip().astype(float)\n",
    "    gpu_metrics[\"power_draw_percent\"] = gpu_metrics[\"power_draw_watts\"] / MAX_WATTS * 100\n",
    "    gpu_metrics[\"utilization_gpu_percent\"] = gpu_metrics[\" utilization.gpu [%]\"].str.rstrip(\"%\").str.strip().astype(float)\n",
    "    gpu_metrics[\"utilization_memory_percent\"] = gpu_metrics[\" utilization.memory [%]\"].str.rstrip(\"%\").str.strip().astype(float)\n",
    "    gpu_metrics[\"temperature_gpu_celsius\"] = gpu_metrics[\" temperature.gpu\"]\n",
    "    return gpu_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpu_metrics_for_condition(condition: Condition):\n",
    "    measurements = condition.get_all_runs()\n",
    "    n_measurements = len(measurements)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_cols = min(3, n_measurements)  # Max 3 columns\n",
    "    n_rows = (n_measurements + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    \n",
    "    # Flatten axes array for easier iteration\n",
    "    if n_measurements == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, measurement in enumerate(measurements):\n",
    "        ax = axes[idx]\n",
    "        gpu_metrics = pre_process_gpu_metrics_for_condition(measurement)\n",
    "        \n",
    "        # Plot metrics\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"power_draw_percent\"], linewidth=1.5, alpha=0.8, label=\"Power Draw (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"utilization_gpu_percent\"], linewidth=1.5, alpha=0.8, label=\"GPU Utilization (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"utilization_memory_percent\"], linewidth=1.5, alpha=0.8, label=\"Memory Utilization (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"temperature_gpu_celsius\"], linewidth=1.5, alpha=0.8, label=\"Temperature (°C)\")\n",
    "        \n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_yticks(range(0, 101, 10))\n",
    "        ax.set_xlabel(\"Sample Index\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.set_title(f\"GPU Usage Over Time - Measurement {measurement.index}\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(measurements), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    # Add main title\n",
    "    fig.suptitle(f\"GPU Metrics: {condition.name} ({condition.tee_mode.value})\", fontsize=14, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89993246",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = exp_throughput_latency.get_condition(\"gemma-3-1b-it\", TEE_Mode.TEE_ON)\n",
    "llama = exp_throughput_latency.get_condition(\"Llama-3.1-8B-Instruct\", TEE_Mode.TEE_ON)\n",
    "mistral = exp_throughput_latency.get_condition(\"Mistral-Small-24B-Instruct-2501\", TEE_Mode.TEE_ON)\n",
    "qwen = exp_throughput_latency.get_condition(\"Qwen3-32B\", TEE_Mode.TEE_ON)\n",
    "\n",
    "plot_gpu_metrics_for_condition(gemma)\n",
    "plot_gpu_metrics_for_condition(llama)\n",
    "plot_gpu_metrics_for_condition(mistral)\n",
    "plot_gpu_metrics_for_condition(qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f155368",
   "metadata": {},
   "source": [
    "## 4. Price of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530add0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

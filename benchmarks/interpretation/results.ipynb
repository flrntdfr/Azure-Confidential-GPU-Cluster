{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d75d0ce",
   "metadata": {},
   "source": [
    "<!-- du4://thèse/cai/results.ipynb?d=20251024?loc=ttum?hPa=1020 -->\n",
    "\n",
    "# Confidential Artificial Intelligence: What's the Catch?\n",
    "### _Performance and costs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from functools import cached_property\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\", context=\"paper\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent folder containing the data\n",
    "data_path = Path(\"data\", \"ko\")  # ← FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experiment:\n",
    "    path: Path\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.path.stem\n",
    "\n",
    "    @cached_property\n",
    "    def conditions(self) -> List[\"Condition\"]:\n",
    "        return [\n",
    "            Condition(q, q.parent.name, q.name == \"tee_on\")\n",
    "            for q in self.path.glob(\"*/*\")\n",
    "            if q.is_dir() and q.name in {\"tee_on\", \"tee_off\"}\n",
    "        ]\n",
    "\n",
    "    def get_all_conditions(self):\n",
    "        return self.conditions\n",
    "\n",
    "    def get_all_conditions_names(self, sort_by_model_size: bool=False):\n",
    "        all_conditions = self.conditions\n",
    "        if sort_by_model_size:\n",
    "            all_conditions_sorted = sorted(all_conditions, key=lambda c: int(c.model_size))\n",
    "            return list(dict.fromkeys(c.name for c in all_conditions_sorted))\n",
    "        else:\n",
    "            return list(dict.fromkeys(c.name for c in all_conditions))\n",
    "\n",
    "    def get_conditions(self, tee_on: bool=False):\n",
    "        return [c for c in self.conditions if c.tee_on == tee_on]\n",
    "\n",
    "    def get_condition(self, name: str, tee_on: bool=False):\n",
    "        return next(\n",
    "            filter(lambda c: c.name == name and c.tee_on == tee_on, self.conditions),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Experiment: {self.name}, Path: {self.path.absolute()}, {len(self.conditions)} conditions: {[c.name for c in self.conditions]}\"\n",
    "\n",
    "@dataclass\n",
    "class Condition:\n",
    "    path: Path\n",
    "    name: str\n",
    "    tee_on: bool\n",
    "\n",
    "    @property\n",
    "    def model_name(self) :\n",
    "        return self.path.parent.name.split(\"_\")\n",
    "\n",
    "    @property\n",
    "    def model_size(self) -> str:\n",
    "        return re.search(r\"(\\d+)[bB]\", \"_\".join(self.model_name)).group(1)\n",
    "\n",
    "    @cached_property\n",
    "    def repetitions(self) -> List[\"Repetition\"]:\n",
    "        repetitions_paths = list(self.path.glob(\"*repetition_*\"))\n",
    "        json_files = sorted([r for r in repetitions_paths if r.suffix == \".json\"])\n",
    "        csv_files = sorted([r for r in repetitions_paths if r.suffix == \".csv\"])\n",
    "\n",
    "        assert len(list(repetitions_paths)) > 0, \"Empty results\"\n",
    "        assert len(json_files) == len(csv_files), f\"Mismatch: {len(json_files)} .json vs. {len(csv_files)} .csv\"\n",
    "\n",
    "        return [\n",
    "            Repetition(\n",
    "                idx, json_file, self.path / f\"{json_file.stem}_power_metrics.csv\"\n",
    "            )\n",
    "            for idx, json_file in enumerate(json_files)\n",
    "        ]\n",
    "\n",
    "    def get_all_repetitions(self) -> List[\"Repetition\"]:\n",
    "        return self.repetitions\n",
    "\n",
    "    def get_repetition(self, index: int) -> \"Repetition\":\n",
    "        return self.repetitions[index]\n",
    "\n",
    "    def get_median_throughput_with_std(self) -> Tuple[float, float]:\n",
    "        output_throughputs = [\n",
    "            rep.get_vllm_key(\"output_throughput\") for rep in self.repetitions\n",
    "        ]\n",
    "        return np.median(output_throughputs), np.std(output_throughputs)\n",
    "\n",
    "@dataclass\n",
    "class Repetition:\n",
    "    index: int\n",
    "    path_vllm_json: Path\n",
    "    path_power_csv: Path\n",
    "\n",
    "    @cached_property\n",
    "    def vllm_results(self) -> dict:\n",
    "        return json.loads(self.path_vllm_json.read_text())\n",
    "\n",
    "    @cached_property\n",
    "    def gpu_metrics(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.path_power_csv)\n",
    "\n",
    "    @property\n",
    "    def dataset(self) -> str:\n",
    "        return self.vllm_results[\"dataset\"]\n",
    "\n",
    "    @property\n",
    "    def model_id(self) -> str:\n",
    "        return self.vllm_results[\"model\"]\n",
    "\n",
    "    @property\n",
    "    def input_length(self) -> int:\n",
    "        return self.vllm_results[\"input_length\"]\n",
    "\n",
    "    @property\n",
    "    def output_length(self) -> int:\n",
    "        return self.vllm_results[\"output_length\"]\n",
    "\n",
    "    @property\n",
    "    def concurrency(self) -> int:\n",
    "        return self.vllm_results[\"concurrency\"]\n",
    "\n",
    "    @property\n",
    "    def temperature(self) -> float:\n",
    "        return self.vllm_results[\"temperature\"]\n",
    "\n",
    "    def get_vllm_key(self, key: str):\n",
    "        return self.vllm_results[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295f99c",
   "metadata": {},
   "source": [
    "## 0. Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Number of run\n",
    "# TODO: Total accumulated time (+ estimated cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93012090",
   "metadata": {},
   "source": [
    "## 1. Throughput and Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c04414",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_throughput_latency = Experiment(data_path.joinpath(\"throughput_latency\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac2bf2",
   "metadata": {},
   "source": [
    "### 1.1. Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc172c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []  # We build row by row\n",
    "for condition in experiment_throughput_latency.get_all_conditions():\n",
    "    repetitions_paths = condition.get_all_repetitions()\n",
    "\n",
    "    for rep in repetitions_paths:\n",
    "        assert all(e == \"\" for e in rep.get_vllm_key(\"errors\")), (\n",
    "            \"vLLM reported an error. Check .json.\"\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                # Repetition\n",
    "                \"condition\": condition.name,\n",
    "                \"tee_on\": condition.tee_on,\n",
    "                \"repetition #\": rep.index,\n",
    "                \"duration (s)\": round(rep.get_vllm_key(\"duration\")),\n",
    "                # Throughput\n",
    "                \"output throughput (tok/s)\": rep.get_vllm_key(\"output_throughput\"),\n",
    "                \"total token throughput (tok/s)\": rep.get_vllm_key(\n",
    "                    \"total_token_throughput\"\n",
    "                ),\n",
    "                # Latency\n",
    "                \"Azure cost (€)\": round(rep.get_vllm_key(\"duration\") / 3600 * 7, 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a648b",
   "metadata": {},
   "source": [
    "### 1.2. Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_labels = []\n",
    "tee_on_medians, tee_on_stds = [], []\n",
    "tee_off_medians, tee_off_stds = [], []\n",
    "\n",
    "for condition in experiment_throughput_latency.get_all_conditions_names(sort_by_model_size=True):\n",
    "    condition_labels.append(condition)\n",
    "    median_tee_on, std_tee_on = experiment_throughput_latency.get_condition(\n",
    "        condition, True\n",
    "    ).get_median_throughput_with_std()\n",
    "    tee_on_medians.append(median_tee_on)\n",
    "    tee_on_stds.append(std_tee_on)\n",
    "    median_tee_off, std_tee_off = experiment_throughput_latency.get_condition(\n",
    "        condition, False\n",
    "    ).get_median_throughput_with_std()\n",
    "    tee_off_medians.append(median_tee_off)\n",
    "    tee_off_stds.append(std_tee_off)\n",
    "\n",
    "# Plot grouped bar chart with error bars\n",
    "x = np.arange(len(condition_labels))\n",
    "width = 0.20  # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(\n",
    "    x - width / 2,\n",
    "    tee_on_medians,\n",
    "    width,\n",
    "    yerr=tee_on_stds,\n",
    "    label=\"TEE On\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax.bar(\n",
    "    x + width / 2,\n",
    "    tee_off_medians,\n",
    "    width,\n",
    "    yerr=tee_off_stds,\n",
    "    label=\"TEE Off\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add labels, title and legend\n",
    "ax.set_xlabel(\"Conditions\")\n",
    "ax.set_ylabel(\"Throughput\")\n",
    "ax.set_title(\"Throughput Comparison: TEE On vs TEE Off\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(condition_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782e86c",
   "metadata": {},
   "source": [
    "### 1.3. Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0b05a",
   "metadata": {},
   "source": [
    "## 2. Saturation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm_data.get(\"max_concurrent_requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ef0d4",
   "metadata": {},
   "source": [
    "## 3. Sequence length overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a576b95",
   "metadata": {},
   "source": [
    "## 5. Energy efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns for plotting\n",
    "def pre_process_gpu_metrics_for_condition(Repetition: rep):\n",
    "    MAX_WATTS = 700\n",
    "    gpu_metrics = rep.gpu_metrics\n",
    "    gpu_metrics[\"power_draw_watts\"] = gpu_metrics[\" power.draw [W]\"].str.rstrip(\"W\").str.strip().astype(float)\n",
    "    gpu_metrics[\"power_draw_percent\"] = gpu_metrics[\"power_draw_watts\"] / MAX_WATTS * 100\n",
    "    gpu_metrics[\"utilization_gpu_percent\"] = gpu_metrics[\" utilization.gpu [%]\"].str.rstrip(\"%\").str.strip().astype(float)\n",
    "    gpu_metrics[\"utilization_memory_percent\"] = gpu_metrics[\" utilization.memory [%]\"].str.rstrip(\"%\").str.strip().astype(float)\n",
    "    gpu_metrics[\"temperature_gpu_celsius\"] = gpu_metrics[\" temperature.gpu\"]\n",
    "    return gpu_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpu_metrics_for_condition(condition: Condition):\n",
    "    repetitions = condition.get_all_repetitions()\n",
    "    n_repetitions = len(repetitions)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_cols = min(3, n_repetitions)  # Max 3 columns\n",
    "    n_rows = (n_repetitions + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    \n",
    "    # Flatten axes array for easier iteration\n",
    "    if n_repetitions == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, repetition in enumerate(repetitions):\n",
    "        ax = axes[idx]\n",
    "        gpu_metrics = pre_process_gpu_metrics_for_condition(repetition)\n",
    "        \n",
    "        # Plot metrics\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"power_draw_percent\"], linewidth=1.5, alpha=0.8, label=\"Power Draw (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"utilization_gpu_percent\"], linewidth=1.5, alpha=0.8, label=\"GPU Utilization (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"utilization_memory_percent\"], linewidth=1.5, alpha=0.8, label=\"Memory Utilization (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"temperature_gpu_celsius\"], linewidth=1.5, alpha=0.8, label=\"Temperature (°C)\")\n",
    "        \n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_yticks(range(0, 101, 10))\n",
    "        ax.set_xlabel(\"Sample Index\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.set_title(f\"GPU Usage Over Time - Repetition {repetition.index}\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(repetitions), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    # Add main title\n",
    "    tee_status = \"TEE On\" if condition.tee_on else \"TEE Off\"\n",
    "    fig.suptitle(f\"GPU Metrics: {condition.name} ({tee_status})\", fontsize=14, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89993246",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = experiment_throughput_latency.get_condition(\"gemma-3-1b-it\", tee_on=True)\n",
    "plot_gpu_metrics_for_condition(condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f155368",
   "metadata": {},
   "source": [
    "## 4. Price of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530add0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

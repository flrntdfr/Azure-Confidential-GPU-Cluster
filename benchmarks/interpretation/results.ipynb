{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d75d0ce",
   "metadata": {},
   "source": [
    "<!-- du4://thèse/cai/results.ipynb?d=20251024?loc=ttum?hPa=1020 -->\n",
    "\n",
    "# Confidential Artificial Intelligence: What's the Catch?\n",
    "### _Performance and costs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from functools import cached_property\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\", context=\"paper\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEE_Mode(Enum):\n",
    "    TEE_ON = \"tee_on\"\n",
    "    TEE_OFF = \"tee_off\"\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    path: Path\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.path.stem\n",
    "\n",
    "    @cached_property\n",
    "    def conditions(self) -> List[\"Condition\"]:\n",
    "        return [\n",
    "            Condition(\n",
    "                q, \n",
    "                q.parent.name, \n",
    "                TEE_Mode.TEE_ON if q.name == \"tee_on\" else TEE_Mode.TEE_OFF\n",
    "            )\n",
    "            for q in self.path.glob(\"*/*\")\n",
    "            if q.is_dir() and q.name in {\"tee_on\", \"tee_off\"}\n",
    "        ]\n",
    "\n",
    "    def get_all_conditions_names(self, sort_by_model_size: bool=False):\n",
    "        all_conditions = self.conditions\n",
    "        if sort_by_model_size:\n",
    "            all_conditions_sorted = sorted(all_conditions, key=lambda c: int(c.model_size))\n",
    "            return list(dict.fromkeys(c.name for c in all_conditions_sorted))\n",
    "        else:\n",
    "            return list(dict.fromkeys(c.name for c in all_conditions))\n",
    "\n",
    "    def get_conditions(self, tee_mode: TEE_Mode):\n",
    "        return [c for c in self.conditions if c.tee_mode == tee_mode]\n",
    "\n",
    "    def get_condition(self, name: str, tee_mode: TEE_Mode):\n",
    "        return next(\n",
    "            filter(lambda c: c.name == name and c.tee_mode == tee_mode, self.conditions),\n",
    "            None,\n",
    "        )\n",
    "    \n",
    "    def get_all_measurements(self):\n",
    "        return [r for c in self.conditions for r in c.measurements]\n",
    "    \n",
    "    def get_measurements(self, tee_mode: TEE_Mode):\n",
    "        return [r for c in self.conditions for r in c.measurements if c.tee_mode == tee_mode]\n",
    "\n",
    "    def __str__(self):\n",
    "        name = self.name\n",
    "        nb_conditions = len(self.conditions)\n",
    "        nb_total_measurements = len(self.get_all_measurements())\n",
    "        return f\"Experiment: {name}, Conditions: {nb_conditions} ({nb_total_measurements} total measurements)\"\n",
    "\n",
    "@dataclass\n",
    "class Condition:\n",
    "    path: Path\n",
    "    name: str\n",
    "    tee_mode: TEE_Mode\n",
    "\n",
    "    @property\n",
    "    def model_name(self) :\n",
    "        return self.path.parent.name.split(\"_\")\n",
    "\n",
    "    @property\n",
    "    def model_size(self) -> str:\n",
    "        return re.search(r\"(\\d+)[bB]\", \"_\".join(self.model_name)).group(1)\n",
    "\n",
    "    @cached_property\n",
    "    def measurements(self) -> List[\"Measurement\"]:\n",
    "        measurements_paths = list(self.path.glob(\"*repetition_*\")) # FIXME repetition -> measurement\n",
    "        json_files = sorted([r for r in measurements_paths if r.suffix == \".json\"])\n",
    "        csv_files = sorted([r for r in measurements_paths if r.suffix == \".csv\"])\n",
    "\n",
    "        assert len(list(measurements_paths)) > 0, \"Empty results\"\n",
    "        assert len(json_files) == len(csv_files), f\"Mismatch: {len(json_files)} .json vs. {len(csv_files)} .csv: {measurements_paths}\"\n",
    "\n",
    "        return [\n",
    "            Measurement(\n",
    "                idx, json_file, self.path / f\"{json_file.stem}_power_metrics.csv\"\n",
    "            )\n",
    "            for idx, json_file in enumerate(json_files)\n",
    "        ]\n",
    "\n",
    "    def get_all_measurements(self) -> List[\"Measurement\"]:\n",
    "        return self.measurements\n",
    "\n",
    "    def get_measurement(self, index: int) -> \"Measurement\":\n",
    "        return self.measurements[index]\n",
    "\n",
    "    def get_median_throughput_with_std(self) -> Tuple[float, float]:\n",
    "        output_throughputs = [\n",
    "            rep.get_vllm_key(\"output_throughput\") for rep in self.measurements\n",
    "        ]\n",
    "        return np.median(output_throughputs), np.std(output_throughputs)\n",
    "\n",
    "@dataclass\n",
    "class Measurement:\n",
    "    index: int\n",
    "    path_vllm_json: Path\n",
    "    path_power_csv: Path\n",
    "\n",
    "    @cached_property\n",
    "    def vllm_results(self) -> dict:\n",
    "        return json.loads(self.path_vllm_json.read_text())\n",
    "\n",
    "    @cached_property\n",
    "    def gpu_metrics(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.path_power_csv)\n",
    "\n",
    "    @property\n",
    "    def dataset(self) -> str:\n",
    "        return self.vllm_results[\"dataset\"]\n",
    "\n",
    "    @property\n",
    "    def model_id(self) -> str:\n",
    "        return self.vllm_results[\"model\"]\n",
    "\n",
    "    @property\n",
    "    def input_length(self) -> int:\n",
    "        return self.vllm_results[\"input_length\"]\n",
    "\n",
    "    @property\n",
    "    def output_length(self) -> int:\n",
    "        return self.vllm_results[\"output_length\"]\n",
    "\n",
    "    @property\n",
    "    def concurrency(self) -> int:\n",
    "        return self.vllm_results[\"concurrency\"]\n",
    "\n",
    "    @property\n",
    "    def temperature(self) -> float:\n",
    "        return self.vllm_results[\"temperature\"]\n",
    "\n",
    "    def get_vllm_key(self, key: str):\n",
    "        return self.vllm_results[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295f99c",
   "metadata": {},
   "source": [
    "## 0. Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent folder containing the data\n",
    "data_path = Path(\"data\", \"calibration\")  # ← FIXME\n",
    "# The experiments\n",
    "exp_throughput_latency = Experiment(data_path.joinpath(\"throughput_latency\"))\n",
    "exp_saturation_point   = Experiment(data_path.joinpath(\"saturation_point\"))\n",
    "exp_sequence_overhead  = Experiment(data_path.joinpath(\"sequence_overhead\"))\n",
    "exp_energy             = Experiment(data_path.joinpath(\"energy\"))\n",
    "# All experiments\n",
    "all_exps = (exp_throughput_latency, exp_saturation_point, exp_sequence_overhead, exp_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_seconds_long(seconds: float) -> str:\n",
    "    h, remainder = divmod(int(seconds), 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "nb_total_measurements = 0\n",
    "duration_total = 0\n",
    "print(f\"• Number of experiments: {len(all_exps)}\")\n",
    "for exp in all_exps:\n",
    "    print(f\"  • {str(exp)}\")\n",
    "    all_measurements = exp.get_all_measurements()\n",
    "    nb_total_measurements += len(all_measurements)\n",
    "    for m in all_measurements:\n",
    "        duration_total += m.get_vllm_key(\"duration\")\n",
    "print(f\"• Total measurements: {nb_total_measurements}\")\n",
    "print(f\"• Total duration: {format_seconds_long(duration_total)}\")\n",
    "print(f\"• Estimated Azure price: {round(duration_total / 3600 * 7, 3)} €\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93012090",
   "metadata": {},
   "source": [
    "## 1. Throughput and Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac2bf2",
   "metadata": {},
   "source": [
    "### 1.1. Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc172c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []  # We build row by row\n",
    "for c in exp_throughput_latency.conditions:\n",
    "    for m in c.measurements:\n",
    "        assert all(e == \"\" for e in m.get_vllm_key(\"errors\")), (\n",
    "            f\"vLLM reported an error during measurement. Check .json {m.path_vllm_json}\"\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                # Condition\n",
    "                \"condition\": c.name,\n",
    "                \"tee_mode\": c.tee_mode.value,\n",
    "                # Measurement\n",
    "                \"Measurement #\": m.index,\n",
    "                \"duration (s)\": round(m.get_vllm_key(\"duration\")),\n",
    "                # Throughput\n",
    "                \"output throughput (tok/s)\": m.get_vllm_key(\"output_throughput\"),\n",
    "                \"total token throughput (tok/s)\": m.get_vllm_key(\n",
    "                    \"total_token_throughput\"\n",
    "                ),\n",
    "                # Latency TODO\n",
    "                # Cloud\n",
    "                \"Azure cost (€)\": round(m.get_vllm_key(\"duration\") / 3600 * 7, 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a648b",
   "metadata": {},
   "source": [
    "### 1.2. Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_labels = []\n",
    "tee_on_medians, tee_on_stds = [], []\n",
    "tee_off_medians, tee_off_stds = [], []\n",
    "\n",
    "for c in exp_throughput_latency.get_all_conditions_names(sort_by_model_size=True):\n",
    "    condition_labels.append(c)\n",
    "    median_tee_on, std_tee_on = exp_throughput_latency.get_condition(\n",
    "        c, TEE_Mode.TEE_ON\n",
    "    ).get_median_throughput_with_std()\n",
    "    tee_on_medians.append(median_tee_on)\n",
    "    tee_on_stds.append(std_tee_on)\n",
    "    median_tee_off, std_tee_off = exp_throughput_latency.get_condition(\n",
    "        c, TEE_Mode.TEE_OFF\n",
    "    ).get_median_throughput_with_std()\n",
    "    tee_off_medians.append(median_tee_off)\n",
    "    tee_off_stds.append(std_tee_off)\n",
    "\n",
    "# Plot grouped bar chart with error bars\n",
    "x = np.arange(len(condition_labels))\n",
    "width = 0.20  # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(\n",
    "    x - width / 2,\n",
    "    tee_on_medians,\n",
    "    width,\n",
    "    yerr=tee_on_stds,\n",
    "    label=\"TEE On\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax.bar(\n",
    "    x + width / 2,\n",
    "    tee_off_medians,\n",
    "    width,\n",
    "    yerr=tee_off_stds,\n",
    "    label=\"TEE Off\",\n",
    "    capsize=5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add labels, title and legend\n",
    "ax.set_xlabel(\"Conditions\")\n",
    "ax.set_ylabel(\"Throughput\")\n",
    "ax.set_title(\"Throughput Comparison: TEE On vs TEE Off\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(condition_labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782e86c",
   "metadata": {},
   "source": [
    "### 1.3. Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0b05a",
   "metadata": {},
   "source": [
    "## 2. Saturation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm_data.get(\"max_concurrent_requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ef0d4",
   "metadata": {},
   "source": [
    "## 3. Sequence length overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a576b95",
   "metadata": {},
   "source": [
    "## 4. Energy efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns for plotting\n",
    "def pre_process_gpu_metrics_for_condition(Measurement: m):\n",
    "    MAX_WATTS = 700\n",
    "    gpu_metrics = m.gpu_metrics\n",
    "    gpu_metrics[\"power_draw_watts\"] = gpu_metrics[\" power.draw [W]\"].str.rstrip(\"W\").str.strip().astype(float)\n",
    "    gpu_metrics[\"power_draw_percent\"] = gpu_metrics[\"power_draw_watts\"] / MAX_WATTS * 100\n",
    "    gpu_metrics[\"utilization_gpu_percent\"] = gpu_metrics[\" utilization.gpu [%]\"].str.rstrip(\"%\").str.strip().astype(float)\n",
    "    gpu_metrics[\"utilization_memory_percent\"] = gpu_metrics[\" utilization.memory [%]\"].str.rstrip(\"%\").str.strip().astype(float)\n",
    "    gpu_metrics[\"temperature_gpu_celsius\"] = gpu_metrics[\" temperature.gpu\"]\n",
    "    return gpu_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpu_metrics_for_condition(condition: Condition):\n",
    "    measurements = condition.get_all_measurements()\n",
    "    n_measurements = len(measurements)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_cols = min(3, n_measurements)  # Max 3 columns\n",
    "    n_rows = (n_measurements + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    \n",
    "    # Flatten axes array for easier iteration\n",
    "    if n_measurements == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, measurement in enumerate(measurements):\n",
    "        ax = axes[idx]\n",
    "        gpu_metrics = pre_process_gpu_metrics_for_condition(measurement)\n",
    "        \n",
    "        # Plot metrics\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"power_draw_percent\"], linewidth=1.5, alpha=0.8, label=\"Power Draw (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"utilization_gpu_percent\"], linewidth=1.5, alpha=0.8, label=\"GPU Utilization (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"utilization_memory_percent\"], linewidth=1.5, alpha=0.8, label=\"Memory Utilization (%)\")\n",
    "        ax.plot(gpu_metrics.index, gpu_metrics[\"temperature_gpu_celsius\"], linewidth=1.5, alpha=0.8, label=\"Temperature (°C)\")\n",
    "        \n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_yticks(range(0, 101, 10))\n",
    "        ax.set_xlabel(\"Sample Index\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.set_title(f\"GPU Usage Over Time - Measurement {measurement.index}\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(measurements), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    # Add main title\n",
    "    fig.suptitle(f\"GPU Metrics: {condition.name} ({condition.tee_mode.value})\", fontsize=14, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89993246",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = exp_throughput_latency.get_condition(\"gemma-3-1b-it\", TEE_Mode.TEE_ON)\n",
    "plot_gpu_metrics_for_condition(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6985407",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = exp_throughput_latency.get_condition(\"Llama-3.1-8B-Instruct\", TEE_Mode.TEE_OFF)\n",
    "plot_gpu_metrics_for_condition(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f155368",
   "metadata": {},
   "source": [
    "## 4. Price of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530add0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

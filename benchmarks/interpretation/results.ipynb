{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d75d0ce",
   "metadata": {},
   "source": [
    "<!-- du4://thèse/cai/results.ipynb?d=20251024?loc=ttum?=hPa=1020 -->\n",
    "\n",
    "# Conventional vs. Confidential Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\", context=\"paper\", font_scale=1.1)\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent folder containing the data\n",
    "data_path = Path(\"data\", \"ko\") # ← FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, path: Path):\n",
    "        self.path: Path = path\n",
    "        self.name: str = path.stem\n",
    "        self.conditions: Dict[str, Condition] = {}\n",
    "        self._set_conditions()\n",
    "\n",
    "    def _set_conditions(self):\n",
    "        conditions = [p for p in self.path.iterdir() if p.is_dir()]\n",
    "        assert len(conditions) > 0, \"No condition found…\"\n",
    "        self.conditions.update(\n",
    "            {c.stem: Condition(c) for c in conditions}\n",
    "        )\n",
    "\n",
    "    def get_all_conditions(self):\n",
    "        return self.conditions.items()\n",
    "    \n",
    "    def get_condition(self, name: str):\n",
    "        return self.conditions[name]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.name}, {self.path.absolute()}, {self.conditions}\"\n",
    "        \n",
    "class Condition:\n",
    "    def __init__(self, path: Path):\n",
    "        self.path: Path = path\n",
    "        self.name: str = path.stem\n",
    "        self.tee_on: bool # TODO\n",
    "        self.dataset: str\n",
    "        self.model: str\n",
    "        self.input_length: int\n",
    "        self.output_length: int\n",
    "        self.concurrency: int\n",
    "        self.temperature: float\n",
    "        self.repetitions: List[Repetition] = []\n",
    "        self._set_self()\n",
    "\n",
    "    def _set_self(self):\n",
    "        repetitions = list(self.path.glob(\"*repetition_*\"))\n",
    "        json_files  = sorted([r for r in repetitions if r.suffix == \".json\"])\n",
    "        csv_files   = sorted([r for r in repetitions if r.suffix == \".csv\"])\n",
    "        \n",
    "        assert len(list(repetitions)) > 0, \"Empty results\"\n",
    "        assert len(json_files) == len(csv_files), f\"Mismatch: {len(json_files)} .json vs. {len(csv_files)} .csv\"\n",
    "        \n",
    "        self.repetitions.extend(\n",
    "            Repetition(idx, json_file, self.path / f\"{json_file.stem}_power_metrics.csv\")\n",
    "            for idx, json_file in enumerate(json_files)\n",
    "        )\n",
    "\n",
    "    def get_all_repetitions(self):\n",
    "        return self.repetitions\n",
    "    \n",
    "    def get_repetition(self, index: int):\n",
    "        return self.repetitions[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Condition: {self.name}, Path: {self.path}, Repetitions: {[r.index for r in self.repetitions]}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class Repetition:\n",
    "    def __init__(self, index: int, vllm_json: Path, power_csv: Path):\n",
    "        # Index\n",
    "        self.index: int = index\n",
    "        # Raw results\n",
    "        self.vllm_json: Path = Path(vllm_json)\n",
    "        self.power_csv: Path = Path(power_csv)\n",
    "        # Parsed results\n",
    "        self.vllm_results = json.loads(vllm_json.read_text())\n",
    "        self.power_results = pd.read_csv(power_csv)\n",
    "  \n",
    "    def get_vllm_results(self):\n",
    "        return self.vllm_results\n",
    "    \n",
    "    def get_power_results(self):\n",
    "        return self.power_results\n",
    "\n",
    "    def get_vllm_key(self, key: str):\n",
    "        return self.vllm_results[key]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Repetition(index={self.index}, json='{self.vllm_json.path}', csv='{self.power_csv.path}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295f99c",
   "metadata": {},
   "source": [
    "# 0. Data summary\n",
    "\n",
    "- Number of run\n",
    "- Total accumulated time (+ estimated cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_seconds_long(seconds: float) -> str:\n",
    "    h, remainder = divmod(int(seconds), 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def get_total_runtime(exp: Experiment):\n",
    "    total_runtime = 0\n",
    "    for condition_name, condition in exp.get_all_conditions():\n",
    "        condition_runtime = 0\n",
    "        repetitions = condition.get_all_repetitions()\n",
    "        for r in repetitions:\n",
    "            duration = r.get_vllm_key(\"duration\")\n",
    "            condition_runtime += duration\n",
    "            total_runtime += duration\n",
    "            print(condition_name, r.index, format_seconds_long(duration))\n",
    "        print(condition_name, format_seconds_long(total_runtime))\n",
    "    print(\"Total runtime\", format_seconds_long(total_runtime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_throughput_latency = Experiment(data_path.joinpath(\"experiment-1\"))\n",
    "get_total_runtime(exp_throughput_latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93012090",
   "metadata": {},
   "source": [
    "## 1. Throughput and Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251133ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(exp: Experiment):\n",
    "    \"\"\" Returs the summary of the experiments results \"\"\"\n",
    "    rows = [] # We build row by row\n",
    "    for condition_name, condition in exp.get_all_conditions():\n",
    "        repetitions = condition.get_all_repetitions()\n",
    "\n",
    "        for rep in repetitions:\n",
    "            # This is a row\n",
    "            vllm_data = rep.get_vllm_results()\n",
    "            assert all(e == \"\" for e in vllm_data.get(\"errors\")), \"vLLM reported an error. Check .json.\"\n",
    "            rows.append({\n",
    "                # Repetition\n",
    "                \"condition\": condition_name,\n",
    "                \"repetition\": rep.index,\n",
    "                \"duration\": vllm_data.get(\"duration\"),\n",
    "                # Throughput\n",
    "                \"output throughput\": vllm_data.get(\"output_throughput\"),\n",
    "                \"total token throughput\": vllm_data.get(\"total_token_throughput\")\n",
    "                # Latency\n",
    "            })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ef41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_throughput_latency = Experiment(data_path.joinpath(\"experiment-1\"))\n",
    "summary_throughput_latency = create_summary(exp_throughput_latency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056870d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for plotting\n",
    "condition_names = []\n",
    "mean_throughputs = []\n",
    "stddev_throughputs = []\n",
    "\n",
    "exp_throughput_latency = Experiment(data_path.joinpath(\"experiment-1\"))\n",
    "for condition_name, condition in exp_throughput_latency.get_all_conditions():\n",
    "    all_output_throughputs = []\n",
    "\n",
    "    for rep in condition.get_all_repetitions():\n",
    "        vllm_data = rep.get_vllm_results()\n",
    "        assert all(e == \"\" for e in vllm_data.get(\"errors\")), \"vLLM reported an error. Check .json.\"\n",
    "        all_output_throughputs.append(vllm_data.get(\"output_throughput\"))\n",
    "\n",
    "    mean_output_throughput = np.mean(all_output_throughputs)\n",
    "    std_dev_output_throughput = np.std(all_output_throughputs)\n",
    "\n",
    "    condition_names.append(condition_name)\n",
    "    mean_throughputs.append(mean_output_throughput)\n",
    "    stddev_throughputs.append(std_dev_output_throughput)\n",
    "\n",
    "# Plotting the bar chart with error bars\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.bar(condition_names, mean_throughputs, yerr=stddev_throughputs, capsize=5, alpha=0.7)\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Mean Output Throughput (tok/s)\")\n",
    "plt.title(\"Outout throughput\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0b05a",
   "metadata": {},
   "source": [
    "## 2. Saturation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm_data.get(\"max_concurrent_requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ef0d4",
   "metadata": {},
   "source": [
    "## 3. Sequence length overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f155368",
   "metadata": {},
   "source": [
    "## 4. Price of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a576b95",
   "metadata": {},
   "source": [
    "## 5. Energy efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530add0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
